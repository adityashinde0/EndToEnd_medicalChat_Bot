{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82a04a6-02e1-4366-bebd-16a9ffba4055",
   "metadata": {},
   "source": [
    "# Cell 1: Environment check (GPU, CUDA, Python)\n",
    "import sys, os, platform, subprocess, textwrap\n",
    "print(\"Python:\", sys.version.splitlines()[0])\n",
    "print(\"Platform:\", platform.platform())\n",
    "# Check NVIDIA + CUDA availability\n",
    "gpu_info = None\n",
    "try:\n",
    "    gpu_info = subprocess.check_output([\"nvidia-smi\", \"-L\"], stderr=subprocess.STDOUT).decode().strip()\n",
    "except Exception as e:\n",
    "    gpu_info = f\"nvidia-smi not found or error: {e}\"\n",
    "print(\"GPU info:\\n\", gpu_info)\n",
    "\n",
    "# We'll check torch after installation in next cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44feb69d-b6f1-4f31-9959-74ed8a44afa2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: Install packages (adjust CUDA version if needed)\n",
    "# Replace cu118 with cu121 or cpu if your system uses a different CUDA version.\n",
    "cuda_tag = \"cu130\"   # change if your driver/CUDA is different\n",
    "index_url = f\"https://download.pytorch.org/whl/{cuda_tag}\"\n",
    "\n",
    "# NOTE: run in the notebook; this will restart kernels if necessary\n",
    "# Use --upgrade to get recent versions; remove --no-deps only if you want pip to manage deps.\n",
    "%pip install --upgrade pip setuptools wheel\n",
    "# Install torch + torchvision compiled for your CUDA\n",
    "%pip install --upgrade --force-reinstall torch torchvision --index-url {index_url}\n",
    "\n",
    "# Other ML / LangChain packages\n",
    "%pip install --upgrade langchain sentence-transformers chromadb pypdf PyMuPDF transformers accelerate safetensors\n",
    "# Optional: bitsandbytes for 8-bit models (only if you plan to use it and your CUDA/toolchain supports it)\n",
    "# %pip install bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c30ee0-5958-4aeb-96cd-3ac859d85a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Verify PyTorch sees the GPU\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "    print(\"Current device name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Current device capability (major.minor):\", torch.cuda.get_device_capability(0))\n",
    "else:\n",
    "    print(\"CUDA not available - check drivers/CUDA toolkit and the wheel you installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bcf362-d97d-48f0-a0d8-4ea04d26bd7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "hf_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=hf_model_name)\n",
    "print(\"Embeddings object created for:\", hf_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807838a4-149c-4176-bafa-3a4b8d6e5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = r\"./healthyheart.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(\"Loaded:\", len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f8a1d-f9e0-49fe-aa52-929066dbd6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\".\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e31775f-fcf8-4ce6-8485-8c3a34776b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load a single PDF\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_path = r\"./healthyheart.pdf\"   # PDF in the same folder as notebook\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(docs)} text chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f197ce-6a4f-4fbf-96eb-8f8653a11e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Create embeddings for chunks (this will use GPU via torch if available)\n",
    "# Create a local directory to persist Chroma\n",
    "persist_dir = \"./chroma_db\"   # change as desired\n",
    "os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "# Compute embeddings and store in Chroma\n",
    "vectordb = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=persist_dir)\n",
    "vectordb.persist()\n",
    "print(\"Chroma vectorstore created and persisted at\", persist_dir)\n",
    "\n",
    "# create retriever\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a7982-eaa0-46ea-8362-f859c499f666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain-classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24985d-e889-46ba-9666-43de57b5ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 ‚Äì RetrievalQA using local Hugging Face model (NO OpenAI / GPT)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "# 1) Choose a HF model (you can change this later if you want)\n",
    "model_name = \"google/flan-t5-base\"   # safe size for RTX 3050\n",
    "\n",
    "# 2) Load tokenizer & model (GPU if available)\n",
    "device = 0 if torch.cuda.is_available() else -1  # 0 = first GPU, -1 = CPU\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "hf_pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# 3) Wrap the HF pipeline as a LangChain LLM\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipe)\n",
    "\n",
    "# 4) Build the RetrievalQA chain (retriever must be created in previous cells)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "# 5) Ask a question over your heart PDF\n",
    "query = \"tell the perfect time table for my healty heart.\"\n",
    "answer = qa.run(query)\n",
    "\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59396832-ae06-48b9-9d7d-83e474a2f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple interactive Q&A loop using an existing `qa` chain\n",
    "\n",
    "print(\"‚úÖ Q&A ready! Ask anything about your PDF.\")\n",
    "print(\"Type 'exit' or 'quit' to stop.\\n\")\n",
    "\n",
    "while True:\n",
    "    question = input(\"üß† Your question: \").strip()\n",
    "\n",
    "    if question.lower() in {\"exit\", \"quit\", \"q\"}:\n",
    "        print(\"üëã Exiting Q&A. Bye!\")\n",
    "        break\n",
    "\n",
    "    if not question:\n",
    "        print(\"Please type a question, or 'exit' to quit.\\n\")\n",
    "        continue\n",
    "\n",
    "    print(\"\\nThinking...\\n\")\n",
    "    try:\n",
    "        answer = qa.run(question)   # `qa` must already be defined in previous cells\n",
    "        print(\"üí¨ Answer:\\n\")\n",
    "        print(answer)\n",
    "        print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error while answering:\", e)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd48827-c9e3-4c69-b604-efdc0b80a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Hugging Face LLM\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "import torch\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.2,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# üî• Create the QA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "print(\"QA chain is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0810453b-1639-4f37-88aa-0f6cc244fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def answer_question(question):\n",
    "    if not question.strip():\n",
    "        return \"Please enter a question.\"\n",
    "    return qa.run(question)   # üî• uses your QA chain\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ü´Ä PDF Question Answering System\")\n",
    "\n",
    "    inp = gr.Textbox(label=\"Ask a Question\")\n",
    "    out = gr.Textbox(label=\"Answer\", lines=10)\n",
    "\n",
    "    btn = gr.Button(\"Get Answer\")\n",
    "    btn.click(answer_question, inputs=inp, outputs=out)\n",
    "\n",
    "demo.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db039fe-0b89-4076-8331-4737c1e65d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def respond(message, history):\n",
    "    \"\"\"\n",
    "    message: latest user question (string)\n",
    "    history: list of [user, bot] pairs (we don't even have to use it for now)\n",
    "    \"\"\"\n",
    "    if not message.strip():\n",
    "        return \"Please enter a question.\"\n",
    "\n",
    "    # RAG answer from your QA chain (stateless per question)\n",
    "    answer = qa.run(message)\n",
    "    return answer\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    title=\"ü´Ä Heart PDF Chatbot\",\n",
    "    description=\"Ask any question about your heart health PDF.\",\n",
    "    examples=[\n",
    "        \"Give me a summary of this document.\",\n",
    "        \"What lifestyle changes does this PDF recommend?\",\n",
    "        \"Explain the risk factors for heart disease mentioned here.\"\n",
    "    ]\n",
    ")\n",
    "def respond(message, history):\n",
    "    if not message.strip():\n",
    "        return \"Please enter a question.\"\n",
    "\n",
    "    # Build a context string from history\n",
    "    convo = \"\"\n",
    "    for user_msg, bot_msg in history:\n",
    "        convo += f\"User: {user_msg}\\nAssistant: {bot_msg}\\n\"\n",
    "    convo += f\"User: {message}\\nAssistant:\"\n",
    "\n",
    "    # Now send the convo as the question to RAG\n",
    "    answer = qa.run(convo)\n",
    "    return answer\n",
    "\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91b7b9-16bc-404f-b841-617adc348312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def respond(message, history):\n",
    "    if not message.strip():\n",
    "        return \"Please enter a question.\"\n",
    "    answer = qa.run(message)\n",
    "    history.append((message, answer))\n",
    "    return \"\", history\n",
    "\n",
    "\n",
    "# üî• Inject custom CSS using HTML (Gradio 4.x compatible)\n",
    "custom_css = \"\"\"\n",
    "<style>\n",
    "body {\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "    background: radial-gradient(circle at top, #1e293b 0, #020617 45%, #000000 100%);\n",
    "    color: #e5e7eb;\n",
    "}\n",
    "\n",
    "/* Main container styling */\n",
    "#chat-container {\n",
    "    max-width: 900px;\n",
    "    margin: 3rem auto;\n",
    "    padding: 25px;\n",
    "    border-radius: 22px;\n",
    "    background: rgba(15, 23, 42, 0.85);\n",
    "    box-shadow: 0 25px 60px rgba(0, 0, 0, 0.65);\n",
    "    border: 1px solid rgba(148, 163, 184, 0.4);\n",
    "    backdrop-filter: blur(18px);\n",
    "}\n",
    "\n",
    "/* Title */\n",
    "h1 {\n",
    "    font-size: 2.2rem !important;\n",
    "    font-weight: 700;\n",
    "    text-align: center;\n",
    "    background: linear-gradient(to right, #22c55e, #38bdf8);\n",
    "    -webkit-background-clip: text;\n",
    "    color: transparent !important;\n",
    "}\n",
    "\n",
    "/* Chat bubbles */\n",
    ".message.user {\n",
    "    background: linear-gradient(135deg, #22c55e, #16a34a);\n",
    "    color: white !important;\n",
    "    border-radius: 16px !important;\n",
    "}\n",
    "\n",
    ".message.bot {\n",
    "    background: rgba(30, 41, 59, 0.8) !important;\n",
    "    border: 1px solid rgba(148, 163, 184, 0.4);\n",
    "    border-radius: 16px !important;\n",
    "}\n",
    "\n",
    "/* Input box */\n",
    "textarea {\n",
    "    background: rgba(15, 23, 42, 0.85) !important;\n",
    "    border-radius: 14px !important;\n",
    "    border: 1px solid rgba(148,163,184,0.6) !important;\n",
    "    color: #e5e7eb !important;\n",
    "}\n",
    "\n",
    "/* Button */\n",
    "button {\n",
    "    border-radius: 999px !important;\n",
    "    font-weight: 600 !important;\n",
    "    background: linear-gradient(135deg, #22c55e, #0ea5e9) !important;\n",
    "    color: white !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    # Add the custom CSS\n",
    "    gr.HTML(custom_css)\n",
    "\n",
    "    gr.HTML(\"<div id='chat-container'>\")\n",
    "\n",
    "    gr.Markdown(\"# ü´Ä Heart Health PDF Chatbot\")\n",
    "    gr.Markdown(\"Ask any question about your PDF!\")\n",
    "\n",
    "    chatbot = gr.Chatbot(height=450)\n",
    "    msg = gr.Textbox(placeholder=\"Ask your question here...\")\n",
    "    submit = gr.Button(\"Send\")\n",
    "\n",
    "    submit.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "\n",
    "    gr.HTML(\"</div>\")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81aa787-f826-4d65-a022-da5ea4876e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CUDA)",
   "language": "python",
   "name": "cuda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
