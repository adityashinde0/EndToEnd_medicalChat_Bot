# ü´Ä Local End-to-End Medical Chatbot

A fully local RAG (Retrieval-Augmented Generation) chatbot that answers questions about medical PDFs using GPU acceleration. Built with LangChain, Hugging Face models, and Gradio.

## ‚ú® Features

- **100% Local Execution**: No API keys required - runs entirely on your machine
- **GPU Accelerated**: Leverages CUDA for fast embedding generation and inference
- **RAG Architecture**: Combines vector search with LLM generation for accurate, context-aware answers
- **Interactive UI**: Beautiful Gradio chat interface with custom styling
- **Medical Domain**: Optimized for health and medical document Q&A

## üéØ Use Cases

- Query medical research papers and health documents
- Get quick answers from lengthy medical PDFs
- Extract specific health information without reading entire documents
- Educational tool for medical students and healthcare professionals

## üõ†Ô∏è Tech Stack

- **LangChain**: Orchestration framework for RAG pipeline
- **Hugging Face Transformers**: Local LLM inference (FLAN-T5)
- **Sentence Transformers**: Document embeddings (all-MiniLM-L6-v2)
- **ChromaDB**: Vector database for semantic search
- **PyTorch**: GPU acceleration
- **Gradio**: Web-based chat interface
- **PyMuPDF/PyPDF**: PDF processing

## üìã Prerequisites

### Hardware Requirements
- **GPU**: NVIDIA GPU with CUDA support (tested on RTX 3050)
- **RAM**: Minimum 8GB (16GB recommended)
- **Storage**: ~5GB for models and dependencies

### Software Requirements
- Python 3.8+
- CUDA Toolkit (11.8, 12.1, or 13.0)
- NVIDIA GPU drivers
- Jupyter Notebook or JupyterLab

## üöÄ Installation

### 1. Clone or Download
```bash
# Place the notebook in your working directory
# Ensure your PDF is in the same folder (e.g., healthyheart.pdf)
```

### 2. Environment Setup
The notebook handles all installations automatically. Key packages:
- PyTorch (CUDA-enabled)
- LangChain ecosystem
- Transformers & Sentence Transformers
- ChromaDB
- Gradio

### 3. Configure CUDA Version
In **Cell 2**, update the CUDA tag if needed:
```python
cuda_tag = "cu130"  # Options: cu118, cu121, cu130, or cpu
```

## üìñ Usage

### Step-by-Step Execution

#### 1. **Environment Check** (Cell 1)
Verifies Python version, platform, and GPU availability
```python
# Checks nvidia-smi output and system info
```

#### 2. **Install Dependencies** (Cell 2)
Installs all required packages with correct CUDA versions
```python
# Automatically installs PyTorch, LangChain, etc.
```

#### 3. **GPU Verification** (Cell 3)
Confirms PyTorch can access your GPU
```python
# Shows CUDA availability and device details
```

#### 4. **Load Embeddings Model** (Cell 4)
Initializes the sentence transformer for document encoding
```python
hf_model_name = "sentence-transformers/all-MiniLM-L6-v2"
```

#### 5. **Process PDF** (Cells 5-6)
Loads PDF, splits into chunks, and creates vector database
```python
pdf_path = "./healthyheart.pdf"  # Update with your PDF path
```

#### 6. **Initialize LLM** (Cell 7)
Sets up the FLAN-T5 model for question answering
```python
model_name = "google/flan-t5-base"  # Lightweight and effective
```

#### 7. **Launch Chatbot** (Final cells)
Starts the Gradio interface for interactive Q&A
```python
demo.launch()  # Opens at http://127.0.0.1:7860
```

## üí¨ Example Questions

Try asking the chatbot:
- "Give me a summary of this document"
- "What lifestyle changes does this PDF recommend?"
- "Explain the risk factors for heart disease mentioned here"
- "What are the symptoms discussed in the document?"
- "Tell me the perfect timetable for a healthy heart"

## üé® UI Features

The Gradio interface includes:
- **Modern Dark Theme**: Gradient background with glassmorphism
- **Smooth Animations**: Polished user experience
- **Message History**: Maintains conversation context
- **Responsive Design**: Works on desktop and tablet screens
- **Custom Styling**: Green/blue gradient accents

## üìù Code Issues Found

### Issue 1: Incorrect Gradio ChatInterface Usage (Final Cell)
**Location:** Last cell with `def respond(message, history):`

**Problem:**
```python
def respond(message, history):
    answer = qa.run(message)
    history.append((message, answer))  # ‚ùå Wrong - don't modify history
    return "", history  # ‚ùå Wrong - don't return history
```

**Fix:**
```python
def respond(message, history):
    if not message.strip():
        return "Please enter a question."
    answer = qa.run(message)
    return answer  # ‚úÖ Just return the answer string
```

**Explanation:** `ChatInterface` automatically manages conversation history. You only need to return the bot's response.

### Issue 2: Duplicate Function Definition
**Location:** Second-to-last cell

The notebook defines `respond()` twice in the same cell, which causes the first definition to be overwritten.

### Issue 3: Commented Code with Old Import
**Location:** Cell 5

```python
# from langchain.text_splitter import RecursiveCharacterTextSplitter  # ‚ùå Old import
from langchain_text_splitters import RecursiveCharacterTextSplitter  # ‚úÖ Current import
```

The old import path is commented out (good!) but should be noted that `langchain.text_splitter` was deprecated and moved to `langchain_text_splitters`.

### Issue 4: Missing Gradio Package in Requirements
The notebook doesn't explicitly install Gradio in Cell 2. Add this:
```python
%pip install --upgrade gradio
```

## üîß Fixes Summary

**To fix the chatbot immediately:**
1. In the final cell, change `respond()` to only return the answer string
2. Remove the line `history.append((message, answer))`
3. Change `return "", history` to `return answer`

**Corrected final cell:**
```python
import gradio as gr

def respond(message, history):
    if not message.strip():
        return "Please enter a question."
    return qa.run(message)

custom_css = """
<style>
/* Your CSS here */
</style>
"""

with gr.Blocks() as demo:
    gr.HTML(custom_css)
    gr.HTML("<div id='chat-container'>")
    gr.Markdown("# ü´Ä Heart Health PDF Chatbot")
    gr.Markdown("Ask any question about your PDF!")
    
    gr.ChatInterface(
        fn=respond,
        title="ü´Ä Heart Health PDF Chatbot",
        description="Ask any question about your heart health PDF.",
        examples=[
            "Give me a summary of this document.",
            "What lifestyle changes does this PDF recommend?",
            "Explain the risk factors for heart disease mentioned here."
        ]
    )
    
    gr.HTML("</div>")

demo.launch()
```

## ‚öôÔ∏è Customization

### Change the PDF
```python
pdf_path = "./your-document.pdf"
```

### Adjust Chunk Size
```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,      # Increase for longer context
    chunk_overlap=150    # Overlap between chunks
)
```

### Use Different Models

**Larger LLM** (requires more VRAM):
```python
model_name = "google/flan-t5-large"  # or flan-t5-xl
```

**Better Embeddings**:
```python
hf_model_name = "sentence-transformers/all-mpnet-base-v2"
```

### Retrieval Settings
```python
retriever = vectordb.as_retriever(search_kwargs={"k": 4})  # Top 4 chunks
```

## ‚ö†Ô∏è Important Notes

### LangChain Version Compatibility
This notebook uses **langchain-classic** which contains legacy chains like `RetrievalQA`. As of LangChain v1.0:
- `RetrievalQA` and other legacy chains have been moved to the `langchain-classic` package
- The `langchain-classic` package will receive security updates until **December 2026**
- For new projects, consider migrating to the modern LangChain v1.0 API with `create_retrieval_chain`

**Current approach (used in notebook):**
```python
from langchain_classic.chains import RetrievalQA
qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)
```

**Modern alternative (recommended for new projects):**
```python
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "Answer using the context: {context}"),
    ("human", "{input}")
])
question_answer_chain = create_stuff_documents_chain(llm, prompt)
chain = create_retrieval_chain(retriever, question_answer_chain)
```

### Gradio ChatInterface History Behavior
The notebook's ChatInterface implementation has an issue in the last cell:
```python
def respond(message, history):
    # This attempts to modify history but doesn't work correctly
    history.append((message, answer))
    return "", history
```

**Problem:** Gradio's `ChatInterface` manages history automatically - you should **not** manually append to it or return it as an output.

**Correct implementation:**
```python
def respond(message, history):
    # history is automatically managed by ChatInterface
    # Just return the answer string
    if not message.strip():
        return "Please enter a question."
    answer = qa.run(message)
    return answer  # Only return the answer, not history
```

The `history` parameter is read-only and provides context. Gradio automatically appends your return value to the conversation.

## üêõ Troubleshooting

### CUDA Not Available
```bash
# Check NVIDIA drivers
nvidia-smi

# Verify CUDA toolkit installation
nvcc --version

# Reinstall PyTorch with correct CUDA version
pip install torch --index-url https://download.pytorch.org/whl/cu130
```

### Gradio Installation Issue
The notebook installs Gradio as a dependency but doesn't specify version. For best compatibility:
```bash
pip install gradio>=4.0.0
```

### Import Errors with LangChain
If you see `ModuleNotFoundError` for chains or retrievers:
```bash
# Install langchain-classic for legacy components
pip install langchain-classic
```

### RetrievalQA Deprecation Warning
The notebook uses the deprecated `RetrievalQA.from_chain_type()`. While it still works via `langchain-classic`, you may see deprecation warnings. This is expected and the code will continue to function until December 2026.
- Use smaller model: `flan-t5-small` instead of `flan-t5-base`
- Reduce `max_new_tokens` in pipeline
- Lower `chunk_size` in text splitter
- Decrease retriever `k` value

### Slow Performance
- Ensure GPU is being used (check Cell 3 output)
- Close other GPU-intensive applications
- Reduce PDF size or number of chunks

### PDF Loading Issues
```python
# Check file path
import os
print(os.path.exists(pdf_path))

# Try absolute path
pdf_path = r"C:\full\path\to\document.pdf"
```

## üìä Performance Metrics

**On RTX 3050 4GB:**
- PDF Loading: ~2-5 seconds
- Embedding Generation: ~10-20 seconds (200 chunks)
- Query Response Time: ~3-5 seconds per question
- Memory Usage: ~2.5GB VRAM

## üîí Privacy & Security

- **All data stays local** - no cloud API calls
- **No data transmission** - completely offline capable
- **HIPAA-friendly** - suitable for sensitive medical documents
- **No tracking** - your queries are never logged externally

## üó∫Ô∏è Roadmap

Future enhancements:
- [ ] Support for multiple PDFs simultaneously
- [ ] Conversation memory/history
- [ ] Citation tracking (show source pages)
- [ ] Export chat history
- [ ] Support for medical images
- [ ] Fine-tuning on medical datasets

## ü§ù Contributing

Suggestions for improvement:
1. Try different embedding models
2. Experiment with prompt engineering
3. Add support for other document formats (DOCX, TXT)
4. Implement semantic caching for faster repeated queries

## üìù License

This project is open source and available for educational and research purposes.

## üôè Acknowledgments

- **LangChain**: RAG framework
- **Hugging Face**: Open-source models
- **Google**: FLAN-T5 model family
- **Gradio**: UI framework
- **ChromaDB**: Vector database

## üìß Support

For issues or questions:
1. Check the troubleshooting section
2. Verify all prerequisites are met
3. Ensure PDF is in correct location
4. Review error messages in notebook output

---

**Built with ‚ù§Ô∏è for local, privacy-preserving medical AI**